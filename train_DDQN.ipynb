{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DDQN for trailer reversing\n",
    "Here we instantiate a simulation of the trailer system, and train a DDQN agent to control it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from DDQN import DDQNhelpers\n",
    "from DDQN.dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "#import DDQN.dqn_model\n",
    "from DDQN.DDQNhelpers import *\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "from gymEnvironments.bicycleEnv import *\n",
    "\n",
    "#Local files. \n",
    "#from Visualize_combination_code import *\n",
    "#from Simulate_combination_code import *\n",
    "#from utility_functions import *\n",
    "\n",
    "#import DDQNhelpers\n",
    "\n",
    "#dqn_model.test_calculate_q_targets(calculate_q_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU should be enough, but feel free to play around with this if you want to.\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "Define an environment that we are to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##System initial conditions. \n",
    "\n",
    "\n",
    "L = 2 #Length rear axis to front axis\n",
    "Ts = 0.2 #Sample interval in seconds. \n",
    "\n",
    "#Position x, y, and heading\n",
    "initState = (5,0, 0)\n",
    "\n",
    "#Yes, the truck is a bicycle these days. \n",
    "truck = BicycleEnv(L,Ts, initState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "The DDQN framework and all helper functions here assume a structure where actions are defined as integers \n",
    "\n",
    "$a_i \\in \\{0,1,\\ldots , N_a-1  \\}$\n",
    "\n",
    "This seems to be for making array operations within the framework easier and more lightweight. This actions are then mapped to something useful the agent can do, in this case a tuple containing steering and velocity like \n",
    "\n",
    "$a_i \\mapsto \\langle v_i, \\delta_i \\rangle$\n",
    "\n",
    "The map is contained within the environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vel: 1 Steering: 0.7853981633974483\n"
     ]
    }
   ],
   "source": [
    "#Draw an element from the map of actions\n",
    "result = truck.action_map[2]\n",
    "lala = truck.action_map[2].vel\n",
    "print(\"Vel:\" ,result.vel,\"Steering:\", result.steeringRad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a system model and run a simulation for a sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation for velocity 1 m/s and steering 45.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "truck_pos_x = []\n",
    "truck_pos_y = []\n",
    "truck_angle = []\n",
    "\n",
    "action = 2\n",
    "\n",
    "print(\"Simulation for velocity\", truck.action_map[action].vel, \"m/s and steering\", np.rad2deg(truck.action_map[action].steeringRad)   )\n",
    "\n",
    "for step_number in range(np.int(1e3)):\n",
    "    \n",
    "        #velocity = -0.1+np.sin(step_number/1e3*np.pi)\n",
    "        #steering_percentage = np.sin(step_number/1e2*np.pi)\n",
    " \n",
    "        \n",
    "        state,_,_  = truck.step(action) \n",
    "        \n",
    "        truck_pos_x.append(state[0])\n",
    "        truck_pos_y.append(state[1])\n",
    "        truck_angle.append(state[2])\n",
    "        #truck_rot.append(truck_rotation)\n",
    "        #trailer1_rot.append(first_trailer_rotation)\n",
    "        #trailer2_rot.append(second_trailer_rotation)\n",
    "\n",
    "\n",
    "#Reset before training on network.         \n",
    "truck.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate data for a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'animateRectangle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b37ec18cb282>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0manimateRectangle\u001b[0m \u001b[1;32mimport\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'notebook '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#The previous code was put in a separate class, try it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'animateRectangle'"
     ]
    }
   ],
   "source": [
    "from animateRectangle import* \n",
    "\n",
    "%matplotlib notebook \n",
    "\n",
    "#The previous code was put in a separate class, try it here. \n",
    "#%matplotlib tk \n",
    "fig= plt.figure()\n",
    "#ax.axis('equal')\n",
    "fig.set_dpi(100)\n",
    "fig.set_size_inches(3, 3)\n",
    "B =0\n",
    "\n",
    "rectAnim = animateRectangle(fig, B,L, truck_pos_x, truck_pos_y, truck_angle)\n",
    "\n",
    "anim = rectAnim.animate(Ts*1000)\n",
    "plt.show()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ddqn( env, ddqn, replay_buffer, num_episodes, enable_visualization=False, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    #Things we log per each time step each episode. \n",
    "    perStepLogTuple = namedtuple(\"perStepLog\", [\"Px\", \"Py\",\"angleRad\", \"action\"])\n",
    "    \n",
    "    #Nesteld tuples \n",
    "    perEpisodeLogTuple = namedtuple(\"perEpLog\", [\"perStepLog\", \"eps\", \"R\", \"Ravg\"])\n",
    "    \n",
    "    trainingLog = perEpisodeLogTuple(perStepLog = [], eps = [], R=[], Ravg=[] )\n",
    "    \n",
    "    #Initial and final probability of taking a random action\n",
    "    eps = 0.99\n",
    "    eps_end = 0.1 \n",
    "    \n",
    "    #Epsilon decays this much each apisode\n",
    "    #eps_decay = .01\n",
    "    eps_decay = 0.002\n",
    "    tau = 1000\n",
    "    cnt_updates = 0\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    \n",
    "    print('Starting to train.')\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() # Initial state\n",
    "        state = state[None,:] # Add singleton dimension, to represent as batch of size 1.\n",
    "        finish_episode = False # Initialize\n",
    "        ep_reward = 0 # Initialize \"Episodic reward\", i.e. the total reward for episode, when disregarding discount factor.\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        maxSteps = 100\n",
    "        \n",
    "        perStepLog = perStepLogTuple(Px =[], Py =[], action =[], angleRad = [])\n",
    "        \n",
    "        while not finish_episode and steps < maxSteps:\n",
    "           \n",
    "            if enable_visualization:\n",
    "                env.render() # comment this line out if you don't want to / cannot render the environment on your system\n",
    "            steps += 1\n",
    "\n",
    "            # Take one step in environment. No need to compute gradients,\n",
    "            # we will just store transition to replay buffer, and later sample a whole batch\n",
    "            # from the replay buffer to actually take a gradient step.\n",
    "            q_online_curr, curr_action = calc_q_and_take_action(ddqn, state, eps)\n",
    "            q_buffer.append(q_online_curr)\n",
    "            \n",
    "            #Velocity is constant for now\n",
    "            new_state, reward, finish_episode,  = env.step(curr_action) # take one step in the evironment\n",
    "            \n",
    "            perStepLog.Px.append(new_state[0] )\n",
    "            perStepLog.Py.append(new_state[1] )\n",
    "            perStepLog.angleRad.append(new_state[2] )\n",
    "            perStepLog.action.append(curr_action)\n",
    "            \n",
    "            #set_trace()\n",
    "            \n",
    "            new_state = new_state[None,:]\n",
    "            \n",
    "            \n",
    "            # Assess whether terminal state was reached.\n",
    "            # The episode may end due to having reached 200 steps, but we should not regard this as reaching the terminal state, and hence not disregard Q(s',a) from the Q target.\n",
    "            # https://arxiv.org/abs/1712.00378\n",
    "            #nonterminal_to_buffer = not finish_episode or steps == maxSteps\n",
    "            \n",
    "            #For this agent the above shoudlnt hold\n",
    "            nonterminal_to_buffer = not finish_episode \n",
    "            \n",
    "            # Store experienced transition to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=curr_action, r=reward, next_s=new_state, t=nonterminal_to_buffer))\n",
    "\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # If replay buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                \n",
    "                loss = sample_batch_and_calculate_loss(ddqn, replay_buffer, batch_size, gamma)\n",
    "                ddqn.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ddqn.optimizer.step()\n",
    "\n",
    "                cnt_updates += 1\n",
    "                if cnt_updates % tau == 0:\n",
    "                    ddqn.update_target_network()\n",
    "        #########################\n",
    "        ## End of episode\n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        \n",
    "        R_buffer.append(ep_reward)\n",
    "         \n",
    "        # Running average of episodic rewards (total reward, disregarding discount factor)\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1])  if i > 0 else  R_avg.append(R_buffer[i])\n",
    "        \n",
    "        trainingLog.perStepLog.append(perStepLog)\n",
    "        trainingLog.eps.append(eps)\n",
    "        trainingLog.Ravg.append(R_avg[-1])\n",
    "        trainingLog.R.append(ep_reward )\n",
    "\n",
    "        if(i%10 == 0):\n",
    "            print('Episode: {:d}, Total Reward (running avg): {:4.0f}, Epsilon: {:.3f}'.format( i, R_avg[-1], eps))\n",
    "        #print('Episode: {:d}, Total Reward (running avg): {:4.0f} ({:.2f}) Epsilon: {:.3f}, Avg Q: {:.4g}'.format(i, ep_reward, R_avg[-1], eps, np.mean(np.array(q_buffer))))\n",
    "        \n",
    "        # If running average > 195 (close to 200), the task is considered solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return trainingLog\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    return trainingLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train.\n",
      "Episode: 0, Total Reward (running avg): -9081, Epsilon: 0.988\n",
      "Episode: 10, Total Reward (running avg): -9298, Epsilon: 0.968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1596e6cde3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#DDQNhelpers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtrainingLog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop_ddqn\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtruck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddqn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_visualization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-187d5408c5a0>\u001b[0m in \u001b[0;36mtrain_loop_ddqn\u001b[1;34m(env, ddqn, replay_buffer, num_episodes, enable_visualization, batch_size, gamma)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# we will just store transition to replay buffer, and later sample a whole batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# from the replay buffer to actually take a gradient step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mq_online_curr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_q_and_take_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddqn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mq_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_online_curr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ProjectEnvironment\\DDQN\\DDQNhelpers.py\u001b[0m in \u001b[0;36mcalc_q_and_take_action\u001b[1;34m(ddqn, state, eps)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m#state_tensor = state_tensor.detach()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mq_online_curr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_model\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstate_tensor\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m#set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PPO\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ProjectEnvironment\\DDQN\\dqn_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_relu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_relu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fc_final\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PPO\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PPO\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PPO\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "#env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Enable visualization? Does not work in all environments.\n",
    "enable_visualization = False\n",
    "\n",
    "#Actions are full turn left, straight, full turn right\n",
    "#actions = (-1,0,1)\n",
    "\n",
    "\n",
    "# Initializations\n",
    "\n",
    "\n",
    "num_actions = len(truck.action_map.keys()) #TODO: Hardcoded now, do something more fancy later.\n",
    "num_states = len(truck.initState)\n",
    "\n",
    "#Training hyperparameters. \n",
    "num_episodes = 700\n",
    "batch_size = 128\n",
    "gamma = .94\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Object holding our online / offline Q-Networks\n",
    "ddqn = DoubleQLearningModel(device, num_states, num_actions, learning_rate)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(device, num_states)\n",
    "\n",
    "# Train\n",
    "#set_trace()\n",
    "#R, R_avg = train_loop_ddqn(ddqn, env, replay_buffer, num_episodes, enable_visualization=enable_visualization, batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "#DDQNhelpers.\n",
    "#DDQNhelpers.\n",
    "#set_trace()\n",
    "trainingLog = train_loop_ddqn( truck, ddqn, replay_buffer, num_episodes, enable_visualization, batch_size, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable the liveplot here\n",
    "%matplotlib inline\n",
    "\n",
    "#print(type(episode))\n",
    "#plt.plot( episode[pos_x], episode[pos_y] )\n",
    "\n",
    "plt.figure() \n",
    "\n",
    "def printEpisode(episode, i, text = None ):\n",
    "    #Plot startpoint\n",
    "    plt.plot(episode.Px[0],episode.Py[0],  'bo' )\n",
    "\n",
    "    #Plot whole trajectory. \n",
    "    if(text == None):\n",
    "        plt.plot( episode.Px, episode.Py, label = \"Episode \" + str(i) )\n",
    "    else:\n",
    "        plt.plot( episode.Px, episode.Py,'k--', label = \"Episode \" + str(i) + text )\n",
    "    #legend()\n",
    "    plt.gca().legend()\n",
    "    plt.plot(episode.Px[-1],episode.Py[-1],  'kx' )\n",
    "    \n",
    "\n",
    "for i, episode in enumerate(trainingLog.perStepLog):\n",
    "    if (i%100==0):\n",
    "        printEpisode(episode,i)\n",
    "    \n",
    "\n",
    "#Find the episode that gave us the highest value \n",
    "\n",
    "maxInd = np.argmax( trainingLog.R  )\n",
    "\n",
    "episodeWithMostReward = trainingLog.perStepLog[maxInd]\n",
    "\n",
    "printEpisode(episodeWithMostReward, maxInd, \" Highest reward\")\n",
    "    \n",
    "plt.figure() \n",
    "\n",
    "plt.plot(-np.log( np.abs(trainingLog.R)))\n",
    "plt.title('Reward')\n",
    "\n",
    "plt.figure() \n",
    "\n",
    "plt.plot(trainingLog.eps*100)\n",
    "plt.title('Epsilon (probability to take a random action)')\n",
    "\n",
    "#for episode in train_log: \n",
    "#plt.plot(episode.steering_angle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the episode with most reward a bit further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the actions taken for the best episode \n",
    "\n",
    "vel = []\n",
    "steeringAngle = []\n",
    " \n",
    "actionTuple = truck.action_map[0]\n",
    "for a in episodeWithMostReward.action: \n",
    "    #Convert action integer number to the tuple of actions we defined. \n",
    "    actionTuple = truck.action_map[a]\n",
    "    vel.append(actionTuple.vel)\n",
    "    steeringAngle.append(actionTuple.steeringRad)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].plot(vel, 'r', label = 'Velocity') #row=0, col=0\n",
    "#ax[1, 0].plot(range(10), 'b') #row=1, col=0\n",
    "ax[1].plot(np.rad2deg(steeringAngle), 'g') #row=0, col=1\n",
    "#ax[1, 1].plot(range(10), 'k') #row=1, col=1\n",
    "\n",
    "plt.show()\n",
    "plt.figure()     \n",
    "#plt.plot(vel)\n",
    "#plt.title('Actions in the episode with most reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from animateRectangle import* \n",
    "\n",
    "%matplotlib notebook \n",
    "\n",
    "#The previous code was put in a separate class, try it here. \n",
    "#%matplotlib tk \n",
    "fig= plt.figure()\n",
    "#ax.axis('equal')\n",
    "fig.set_dpi(100)\n",
    "fig.set_size_inches(3, 3)\n",
    "B =0\n",
    "\n",
    "log = episodeWithMostReward\n",
    "\n",
    "\n",
    "rectAnim = animateRectangle(fig, B,L, log.Px, log.Py, log.angleRad)\n",
    "\n",
    "anim = rectAnim.animate(Ts*1000)\n",
    "plt.show()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = trainingLog.perStepLog[0].Px\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
